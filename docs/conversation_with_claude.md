think harder: 在 @app/ai/ 下的llm目录下， 实现对如openai,claude等不同厂商不同大模型调用的封装，要求满足：
1.支持不同厂商的调用，如openai,claude等；
2.支持流式、非流式两种模式的响应；
3.对于推理类的模型，自动进行推理过程和答案的拆分；
4.对模型不同响应结果的消息封装，如tools等
5.对system prompt和用户prompt或messages的自动处理

我上面说的不一定更合理，你先和我进行充分沟通讨论，有任何疑问都可以问我，等讨论明确再按照我的要求进行下一步

针对你反馈的几个点，我再详细说明下：
反馈1. 指会返回reasoning_content的模型，感觉直接兼容reasoning_content字段就可以了
反馈2. 只做基础的tool use请求封装
反馈3. 我本意是希望业务逻辑中不需要额外处理systemprompt和消息的拼接，由封装的模型实例处理，自动把systemprompt和消息拼接好，然后调用模型接口
反馈4. 结构设计OK，实现时要考虑，很多厂商实验openai的包也能执行
反馈5. 错误、重试、计费、超时都统一封装和处理，模型别名暂不需要

我这搭建的是模型调用基础能力，希望支持各种场景的模型调用。
Tool Use比如用户说要看某个简历，就需要对应简历的tool调用，但是tool调用不在LLM调用封装中，而是在更上一层的能力封装。这层调度llm的封装能力和tool的执行
我希望满足场景主要场景的模型调用，这里其实就是对大模型的基础调用，基模接口本身就包含了这些能力，只是为业务提供跨厂商和模型的封装，整体还是遵循简单明了、少即是多的思想
可以先实现openai、火山的模型
有问题可以继续和我沟通


针对你的几个疑问，我详细说明下：
疑问1： 我也倾向方案B，支持不同场景下不同的模型
疑问2： LLM封装不用关系模型名，也不需要处理
疑问3： 和模型厂商的实现保持一致
疑问4： 只需要异步

先别着急执行，还有些细节需确认：
1. tool_call需要单独的字段吗，我看现在很多模型厂商都是和消息一起返回的，只是用不同的type区分，这个你在查下资料确认下，看如何实现更好
2. openai、火山现在都支持Response API，你觉得直接用这个，还是response api 和chatcompletions都支持

我觉得更好是遵循模型厂商